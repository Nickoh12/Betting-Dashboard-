{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Pinnacle Odds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import datetime #as dt\n",
    "from datetime import date \n",
    "from datetime import datetime\n",
    "import pandas as pd \n",
    "import re \n",
    "import numpy as np\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support.expected_conditions import presence_of_element_located\n",
    "import sys\n",
    "import warnings\n",
    "from datetime import timedelta\n",
    "import time as timp \n",
    "import dateutil.parser\n",
    "import mysql.connector\n",
    "import sqlalchemy\n",
    "from mysql.connector import Error\n",
    "\n",
    "if not sys.warnoptions:\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    \n",
    "#Get df of leagues \n",
    "\n",
    "def get_pinn_leagues(URL):\n",
    "    chrome_path= r'C:\\Users\\Iris\\Documents\\Nico\\NicoUni\\Practicum\\Python\\Projects\\WebDrivers\\chromedriver.exe'\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument('--headless')\n",
    "    browser = webdriver.Chrome(\n",
    "        executable_path=chrome_path, options=chrome_options)\n",
    "    browser.get(URL)\n",
    "    #browser.implicitly_wait(20)\n",
    "\n",
    "    timp.sleep(20)\n",
    "\n",
    "    htmlSource = browser.page_source\n",
    "    #browser.implicitly_wait(20)\n",
    "\n",
    "    soup= BeautifulSoup(htmlSource, 'lxml')\n",
    "    browser.quit()\n",
    "    league_name= []\n",
    "    links= []\n",
    "    for element in soup.select('a[class*=\"style_supportFavoritesList\"]'):\n",
    "        links.append('https://www.pinnacle.com'+element['href'])\n",
    "        league_name.append(element.text)\n",
    "    \n",
    "    link_frame= pd.DataFrame({\n",
    "        'league': league_name,\n",
    "        'link': links})\n",
    "    \n",
    "    p= re.compile('.*[^0-9]')\n",
    "        \n",
    "    for i in np.arange(0,len(link_frame)):\n",
    "        link_frame['league'][i]= p.search(link_frame['league'][i]).group()\n",
    "        \n",
    "    return(link_frame)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pinn_odds(URL,which):\n",
    "    #import time as timp\n",
    "    chrome_path= r'C:\\Users\\Iris\\Documents\\Nico\\NicoUni\\Practicum\\Python\\Projects\\WebDrivers\\chromedriver.exe'\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument('--headless')\n",
    "    browser = webdriver.Chrome(executable_path=chrome_path, options=chrome_options)\n",
    "    #browser.implicitly_wait(10)\n",
    "    browser.get(URL)\n",
    "    timp.sleep(5)\n",
    "    #browser.find_element_by_tag_name(\"button\")\n",
    "    soup= BeautifulSoup(browser.page_source, 'lxml')\n",
    "    browser.quit()\n",
    "    df= pd.DataFrame()\n",
    "    error= []\n",
    "\n",
    "    df= pd.DataFrame()\n",
    "    \n",
    "    for element in soup.find_all('div',{'class':['style_dateBar__3gX3j','style_dateBar__2PS4O']}):\n",
    "        count= len(element.text)\n",
    "        if 'Today' in element.text:\n",
    "            date= pd.to_datetime(datetime.now(), format=\"%d/%m/%Y\")\n",
    "        if 'Tomorrow' in element.text:\n",
    "            days_add= timedelta(days= 1)\n",
    "            date= (pd.to_datetime(datetime.now(),format=\"%d/%m/%Y\")+days_add)\n",
    "        else:\n",
    "            if which== 'yes':\n",
    "                try:\n",
    "                    temp_date= dateutil.parser.parse(element.text.split(',',1)[1][1:].replace(',',''))\n",
    "                    date= pd.to_datetime(temp_date, format= '%d/%m/%Y')\n",
    "                except:\n",
    "                    date= element.text\n",
    "            else:\n",
    "                 pass\n",
    "        for tag in element.next_siblings:\n",
    "            if tag['class'] == ['style_dateBar__3gX3j'] or tag['class']==['style_dateBar__2PS4O']:\n",
    "                break\n",
    "            if tag['class']== ['style_row__2ww2Y'] or tag['class']== ['style_row__2ww2Y', 'style_lastRow__3Y2le']\\\n",
    "            or tag['class']== ['style_row__3_aBC'] or tag['class']== ['style_row__3_aBC', 'style_lastRow__3h8Pm']:\n",
    "                try:\n",
    "                    teams= tag.find_all('span',{'class':[\"style_participantName__2tjoj ellipsis\", \"style_participantName__CNiJz ellipsis\"]})\n",
    "                    play_time= tag.find('span',{'class':[\"style_time__2GvnE ellipsis\",\"style_time__1_zpO ellipsis\"]})\n",
    "                    odds= tag.find_all('span',{'class':'price'})\n",
    "                    fest= pd.DataFrame({'date':date,\n",
    "                                    'scrape_time':pd.to_datetime(datetime.now(), format=\"%d/%m/%Y %H:%M\"),\n",
    "                                    'time':play_time.text,\n",
    "                         'home_team': teams[0].text,\n",
    "                         'away_team': teams[1].text,\n",
    "                         'PinnacleHome': odds[0].text,\n",
    "                         'PinnacleDraw': odds[1].text,\n",
    "                         'PinnacleAway': odds[2].text}, index= pd.Series(0))\n",
    "                    \n",
    "                    df= df.append(fest)                        \n",
    "                except:\n",
    "                    continue\n",
    "    return(df.reset_index(drop= True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Very Important script\n",
    "#links= get_pinn_leagues('https://www.pinnacle.com/en/soccer/leagues')\n",
    "links= pd.read_csv('links.csv')\n",
    "results= pd.DataFrame()\n",
    "errors= []\n",
    "\n",
    "for i,element in enumerate(links['link']):\n",
    "    try:\n",
    "        df= get_pinn_odds(element,'yes')\n",
    "        df['league']= links['league'][i]\n",
    "        results= results.append(df)       \n",
    "    except:\n",
    "        errors.append(element) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.loc[results['time']== '24:00','time']= '00:00'\n",
    "results.loc[results['time']== '24:15','time']= '00:15'\n",
    "results.loc[results['time']== '24:30','time']= '00:30'\n",
    "results.loc[results['time']== '24:45','time']= '00:45'\n",
    "\n",
    "\n",
    "def change_date(x):\n",
    "    if x == 'Today':\n",
    "        return(datetime.strftime(datetime.now(), format=\"%Y-%m-%d\"))\n",
    "    else:\n",
    "        return(datetime.strftime(x, '%Y-%m-%d'))   \n",
    "\n",
    "\n",
    "results.scrape_time = results.scrape_time.apply(lambda x: x.strftime(format= \"%Y-%m-%d %H:%M\"))\n",
    "results.scrape_time= pd.to_datetime(results.scrape_time)\n",
    "results.date= results.date.map(change_date)\n",
    "results.date= pd.to_datetime(results.date.astype(str)+' '+results.time.astype(str))\n",
    "results['difference_to_scrape']= results.date-results.scrape_time\n",
    "results.difference_to_scrape= results.difference_to_scrape.dt.total_seconds()/3600\n",
    "results= results.drop('time',axis=1)\n",
    "results[['PinnacleHome','PinnacleDraw','PinnacleAway']]= results[['PinnacleHome','PinnacleDraw','PinnacleAway']].astype('float')\n",
    "\n",
    "probs_with_margin= (1/results['PinnacleHome']+1/results['PinnacleDraw']+1/results['PinnacleAway'])\n",
    "\n",
    "results['ProbsPinnacleHome']= 1/results['PinnacleHome']/probs_with_margin\n",
    "results['ProbsPinnacleDraw']= 1/results['PinnacleDraw']/probs_with_margin\n",
    "results['ProbsPinnacleAway']= 1/results['PinnacleAway']/probs_with_margin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Oddschecker Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_odds(link):\n",
    "    try:\n",
    "        URL= link\n",
    "        hdr = {'User-Agent': 'Mozilla/5.0'}\n",
    "        page= requests.get(URL, headers= hdr)\n",
    "        maps= page.text \n",
    "        soup= BeautifulSoup(maps, 'lxml')\n",
    "        \n",
    "# Bookmaker Names\n",
    "        names= []\n",
    "        for element in soup.find_all('a',{\"class\": \"bk-logo-main-90 bk-logo-click\"}):\n",
    "            names.append(element['title'])\n",
    "    \n",
    "        names.insert(0, 'Outcome')\n",
    "\n",
    "# The odds and Team Names \n",
    "        odds= []\n",
    "        teams= []\n",
    "\n",
    "        soup_odds= soup.find('tbody')\n",
    "\n",
    "        for row in soup_odds.find_all('tr'):\n",
    "            cells= row.find_all('td')\n",
    "            outcome= cells[0].find('span').find('span')['data-name']\n",
    "            opus= [x['data-odig'] for x in cells if x.has_attr('data-odig')]\n",
    "            opus.insert(0, outcome)\n",
    "            odds.append(opus)\n",
    "            teams.append(outcome)\n",
    "\n",
    "        date= soup.find('div',{'class':'event'}).text\n",
    "        fixture= soup.find('h1').text\n",
    "        scrape_time= pd.to_datetime(datetime.now(), format=\"%d/%m/%Y %H:%M:%S\")\n",
    "\n",
    "        data= pd.DataFrame(odds, columns= names)\n",
    "\n",
    "        p= re.compile('^.+?(?=\\ v)')\n",
    "\n",
    "        home_team= p.search(fixture).group(0)#[:-1]\n",
    "\n",
    "        lop= list(data['Outcome'])\n",
    "\n",
    "        if home_team== lop[0] and lop[1] != 'Draw':\n",
    "            order= [1,2,0]\n",
    "            lop= [lop[i] for i in order]\n",
    "\n",
    "        if home_team== lop[0] and lop[1] == 'Draw':\n",
    "            order= [2,1,0]\n",
    "            lop= [lop[i] for i in order]\n",
    "            \n",
    "        if home_team== lop[1] and lop[2] == 'Draw':\n",
    "            order= [0,2,1]\n",
    "            lop= [lop[i] for i in order]\n",
    "            \n",
    "        if home_team== lop[1] and lop[2] != 'Draw':\n",
    "            order= [2,0,1]\n",
    "            lop= [lop[i] for i in order]\n",
    "            \n",
    "        if home_team== lop[2] and lop[1] == 'Draw':\n",
    "            order= [0,1,2]\n",
    "            lop= [lop[i] for i in order]\n",
    "            \n",
    "        if home_team== lop[2] and lop[1] != 'Draw':\n",
    "            order= [1,0,2]\n",
    "            lop= [lop[i] for i in order]\n",
    "    \n",
    "        data['Outcome']= pd.Categorical(data['Outcome'], lop)\n",
    "        data= data.sort_values('Outcome').reset_index(drop= True)\n",
    "\n",
    "        a_list= list(data.columns[1:])\n",
    "        b_list= ['H','D','A']\n",
    "\n",
    "        oo= ['{}{}'.format(a,b) for b in b_list for a in a_list]\n",
    "\n",
    "        oo.sort()\n",
    "\n",
    "        for i in np.arange(0,len(oo), 3):\n",
    "            home= oo[i+2]\n",
    "            away= oo[i]\n",
    "            oo[i]= home\n",
    "            oo[i+2]= away\n",
    " \n",
    "        oo.insert(0, 'scrape_time')\n",
    "        oo.insert(0, 'date')\n",
    "        oo.insert(0, 'away_team')\n",
    "        oo.insert(0,'home_team')\n",
    "\n",
    "        new= pd.DataFrame(columns= oo, index= np.arange(0,1))\n",
    "\n",
    "        new['home_team']= data['Outcome'][2]\n",
    "        new['away_team']= data['Outcome'][0]\n",
    "\n",
    "        date= soup.find('div',{'class':'event'}).text.split(',',1)[1][1:].\\\n",
    "        lower().replace(\"rd\", \"\").replace(\"nd\", \"\").replace(\"st\", \"\").replace(\"th\", \"\")\n",
    "\n",
    "        num= [i for i,x in enumerate(date) if x.isspace()][1]\n",
    "\n",
    "        date= date[:num]+ ' '+ str(scrape_time.year) + date[num:]\n",
    "\n",
    "        date= pd.to_datetime(datetime.strptime(date, '%d %B %Y %H:%M'),format= \"%Y-%m-d %H:%M\")\n",
    "        hours_added = timedelta(hours = 1)\n",
    "        new['date']= date+hours_added\n",
    "        new['scrape_time']= scrape_time\n",
    "\n",
    "        for name, stuff in data.drop('Outcome',axis=1).iteritems():\n",
    "            for snam, snugg in new.iteritems():\n",
    "                if name in snam and snam.endswith('A'):\n",
    "                    snugg[0]= stuff[0]\n",
    "                if name in snam and snam.endswith('D'):\n",
    "                    snugg[0]= stuff[1]         \n",
    "                if name in snam and snam.endswith('H'):\n",
    "                    snugg[0]= stuff[2]\n",
    "            \n",
    "        new[new.columns.difference(['home_team', 'away_team','date','scrape_time'])]= new[new.columns.difference(['home_team', 'away_team','date','scrape_time'])].astype('float')\n",
    "        return(new)\n",
    "    \n",
    "    except:\n",
    "        new= pd.DataFrame()\n",
    "        return(new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_leagues(link):\n",
    "    lan= open('l√§nder.txt','r').readlines()\n",
    "    land= [x[:-1] for x in lan]\n",
    "    URL= link\n",
    "    hdr = {'User-Agent': 'Mozilla/5.0'}\n",
    "    page= requests.get(URL, headers= hdr)\n",
    "    maps= page.text \n",
    "    soup= BeautifulSoup(maps, 'lxml')\n",
    "    links= []\n",
    "    for element in soup.find_all('loc'):\n",
    "        s= element.text\n",
    "        if s.count('/') < 6 and any(thing in s for thing in land) and not s.endswith('winner'):\n",
    "            links.append(s)\n",
    "    return(links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors= []\n",
    "oddschecker= pd.DataFrame()\n",
    "links= get_leagues('https://www.oddschecker.com/sport/football/sitemap.xml')\n",
    "for thing in links:\n",
    "    hdr = {'User-Agent': 'Mozilla/5.0'}\n",
    "    page= requests.get(thing, headers= hdr)\n",
    "    maps= page.text \n",
    "    soup= BeautifulSoup(maps, 'lxml')\n",
    "    if len(soup.findAll('td',{'class':\"bh-date beta-caption2\"})) > 2:  \n",
    "        splitter= str(soup.findAll('td',{'class':\"bh-date beta-caption2\"})[2])\n",
    "        soup= BeautifulSoup(str(soup).split(splitter)[0])    \n",
    "    league= thing[thing.rindex('/')+1:]\n",
    "    for element in soup.find_all('a',{'class':'beta-callout full-height-link whole-row-link'}):\n",
    "        try:\n",
    "            new= scrape_odds('https://www.oddschecker.com'+element['href'])\n",
    "            new['league']= league\n",
    "            oddschecker= oddschecker.append(new)\n",
    "        except:\n",
    "            errors.append(element) \n",
    "            \n",
    "oddschecker= oddschecker.reset_index(drop= True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make column names the same \n",
    "oddschecker= oddschecker.sort_values('date').reset_index(drop= True)\n",
    "results= results.sort_values('date').reset_index(drop= True)\n",
    "\n",
    "for i in range(0, len(oddschecker)):\n",
    "    for j in range (0,len(results)):\n",
    "        if (oddschecker['date'][i]== results['date'][j]):\n",
    "            if (oddschecker['home_team'][i] in results['home_team'][j])\\\n",
    "            | (results['home_team'][j] in oddschecker['home_team'][i])\\\n",
    "            | (results['away_team'][j] in oddschecker['away_team'][i])\\\n",
    "            | (oddschecker['away_team'][i] in results['away_team'][j]):\n",
    "                results['home_team'][j]= oddschecker['home_team'][i]\n",
    "                results['away_team'][j]= oddschecker['away_team'][i]\n",
    "                \n",
    "odds_merged= oddschecker.merge(results,on= ['date','home_team','away_team'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "odds_merged= odds_merged.drop_duplicates(subset= ['home_team','away_team']).reset_index(drop= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "bookies= ['10BetH', '10BetD',\n",
    "       '10BetA', '888sportH', '888sportD', '888sportA', 'Bet VictorH',\n",
    "       'Bet VictorD', 'Bet VictorA', 'Bet365H', 'Bet365D', 'Bet365A',\n",
    "       'Betfair SportsbookH', 'Betfair SportsbookD', 'Betfair SportsbookA',\n",
    "       'BetfairH', 'BetfairD', 'BetfairA', 'BetfredH', 'BetfredD', 'BetfredA',\n",
    "       'BetwayH', 'BetwayD', 'BetwayA', 'Boyle SportsH', 'Boyle SportsD',\n",
    "       'Boyle SportsA', 'GentingBetH', 'GentingBetD', 'GentingBetA',\n",
    "       'Mansion BetH', 'Mansion BetD', 'Mansion BetA', 'MatchbookH',\n",
    "       'MatchbookD', 'MatchbookA', 'NovibetH', 'NovibetD', 'NovibetA',\n",
    "       'Paddy PowerH', 'Paddy PowerD', 'Paddy PowerA', 'RedzoneH', 'RedzoneD',\n",
    "       'RedzoneA', 'SkybetH', 'SkybetD', 'SkybetA', 'Smarkets SportsbookH',\n",
    "       'Smarkets SportsbookD', 'Smarkets SportsbookA', 'SmarketsH',\n",
    "       'SmarketsD', 'SmarketsA', 'Sport NationH', 'Sport NationD',\n",
    "       'Sport NationA', 'Sporting IndexH', 'Sporting IndexD',\n",
    "       'Sporting IndexA', 'SpreadexH', 'SpreadexD', 'SpreadexA', 'UnibetH',\n",
    "       'UnibetD', 'UnibetA', 'VBetH', 'VBetD', 'VBetA', 'William HillH',\n",
    "       'William HillD', 'William HillA']\n",
    "\n",
    "exchanges2= ['Betfair SportsbookH','Betfair SportsbookD','Betfair SportsbookA','BetfairH','BetfairD','BetfairA',\\\n",
    "            'MatchbookH','MatchbookD','MatchbookA','SmarketsH','SmarketsD','SmarketsA','Smarkets SportsbookH','Smarkets SportsbookD','Smarkets SportsbookA']\n",
    "\n",
    "\n",
    "exchanges= ['BetfairH','BetfairD','BetfairA',\\\n",
    "            'MatchbookH','MatchbookD','MatchbookA','SmarketsH','SmarketsD','SmarketsA']\n",
    "homes= [x for x in bookies if x.endswith('H')]\n",
    "draws= [x for x in bookies if x.endswith('D')]\n",
    "away= [x for x in bookies if x.endswith('A')]\n",
    "\n",
    "homes2= [x for x in bookies if x.endswith('H') and x not in exchanges]\n",
    "draws2= [x for x in bookies if x.endswith('D') and x not in exchanges]\n",
    "away2= [x for x in bookies if x.endswith('A') and x not in exchanges ]\n",
    "\n",
    "homes3= [x for x in bookies if x.endswith('H') and x in exchanges]\n",
    "draws3= [x for x in bookies if x.endswith('D') and x in exchanges]\n",
    "away3= [x for x in bookies if x.endswith('A') and x in exchanges ]\n",
    "\n",
    "homes4= [x for x in bookies if x.endswith('H') and x not in exchanges2]\n",
    "draws4= [x for x in bookies if x.endswith('D') and x not in exchanges2]\n",
    "away4= [x for x in bookies if x.endswith('A') and x not in exchanges2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "odds_merged['time_lag']= (odds_merged.scrape_time_x - odds_merged.scrape_time_y).dt.total_seconds()\n",
    "odds_merged['max_oddsH']= odds_merged[homes].max(axis= 1)\n",
    "odds_merged['max_oddsD']= odds_merged[draws].max(axis= 1)\n",
    "odds_merged['max_oddsA']= odds_merged[away].max(axis= 1)\n",
    "\n",
    "odds_merged['max_oddsH2']= odds_merged[homes2].max(axis= 1)\n",
    "odds_merged['max_oddsD2']= odds_merged[draws2].max(axis= 1)\n",
    "odds_merged['max_oddsA2']= odds_merged[away2].max(axis= 1)\n",
    "\n",
    "odds_merged['max_oddsH3']= odds_merged[homes3].max(axis= 1)\n",
    "odds_merged['max_oddsD3']= odds_merged[draws3].max(axis= 1)\n",
    "odds_merged['max_oddsA3']= odds_merged[away3].max(axis= 1)\n",
    "\n",
    "odds_merged['max_oddsH4']= odds_merged[homes4].max(axis= 1)\n",
    "odds_merged['max_oddsD4']= odds_merged[draws4].max(axis= 1)\n",
    "odds_merged['max_oddsA4']= odds_merged[away4].max(axis= 1)\n",
    "\n",
    "odds_merged['max_BookieH']= odds_merged[homes].idxmax(axis=1)\n",
    "odds_merged['max_BookieD']= odds_merged[draws].idxmax(axis= 1)\n",
    "odds_merged['max_BookieA']= odds_merged[away].idxmax(axis= 1)\n",
    "\n",
    "odds_merged['max_BookieH2']= odds_merged[homes2].idxmax(axis=1)\n",
    "odds_merged['max_BookieD2']= odds_merged[draws2].idxmax(axis= 1)\n",
    "odds_merged['max_BookieA2']= odds_merged[away2].idxmax(axis= 1)\n",
    "\n",
    "odds_merged['max_BookieH3']= odds_merged[homes3].idxmax(axis=1)\n",
    "odds_merged['max_BookieD3']= odds_merged[draws3].idxmax(axis= 1)\n",
    "odds_merged['max_BookieA3']= odds_merged[away3].idxmax(axis= 1)\n",
    "\n",
    "odds_merged['max_BookieH4']= odds_merged[homes4].idxmax(axis=1)\n",
    "odds_merged['max_BookieD4']= odds_merged[draws4].idxmax(axis= 1)\n",
    "odds_merged['max_BookieA4']= odds_merged[away4].idxmax(axis= 1)\n",
    "\n",
    "odds_merged['marginH']= odds_merged['ProbsPinnacleHome']*(odds_merged['max_oddsH']-1)-(1-odds_merged['ProbsPinnacleHome'])\n",
    "odds_merged['marginD']= odds_merged['ProbsPinnacleDraw']*(odds_merged['max_oddsD']-1)-(1-odds_merged['ProbsPinnacleDraw'])\n",
    "odds_merged['marginA']= odds_merged['ProbsPinnacleAway']*(odds_merged['max_oddsA']-1)-(1-odds_merged['ProbsPinnacleAway'])\n",
    "\n",
    "odds_merged['marginH2']= odds_merged['ProbsPinnacleHome']*(odds_merged['max_oddsH2']-1)-(1-odds_merged['ProbsPinnacleHome'])\n",
    "odds_merged['marginD2']= odds_merged['ProbsPinnacleDraw']*(odds_merged['max_oddsD2']-1)-(1-odds_merged['ProbsPinnacleDraw'])\n",
    "odds_merged['marginA2']= odds_merged['ProbsPinnacleAway']*(odds_merged['max_oddsA2']-1)-(1-odds_merged['ProbsPinnacleAway'])\n",
    "\n",
    "odds_merged['marginH3']= odds_merged['ProbsPinnacleHome']*(odds_merged['max_oddsH3']-1)-(1-odds_merged['ProbsPinnacleHome'])\n",
    "odds_merged['marginD3']= odds_merged['ProbsPinnacleDraw']*(odds_merged['max_oddsD3']-1)-(1-odds_merged['ProbsPinnacleDraw'])\n",
    "odds_merged['marginA3']= odds_merged['ProbsPinnacleAway']*(odds_merged['max_oddsA3']-1)-(1-odds_merged['ProbsPinnacleAway'])\n",
    "\n",
    "odds_merged['marginH4']= odds_merged['ProbsPinnacleHome']*(odds_merged['max_oddsH4']-1)-(1-odds_merged['ProbsPinnacleHome'])\n",
    "odds_merged['marginD4']= odds_merged['ProbsPinnacleDraw']*(odds_merged['max_oddsD4']-1)-(1-odds_merged['ProbsPinnacleDraw'])\n",
    "odds_merged['marginA4']= odds_merged['ProbsPinnacleAway']*(odds_merged['max_oddsA4']-1)-(1-odds_merged['ProbsPinnacleAway'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Data to SQL-Server (Backup CSV Script)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#floats= odds_merged.select_dtypes('float').columns\n",
    "#datetimes= odds_merged.select_dtypes('datetime').columns\n",
    "\n",
    "#odds_merged[datetimes]= odds_merged[datetimes].astype('str')\n",
    "#odds_merged[floats]= odds_merged[floats].astype('float16')\n",
    "\n",
    "#old= pd.read_csv('odds_merged.csv')\n",
    "\n",
    "#new= old.append(odds_merged)\n",
    "#new.to_csv('odds_merged.csv',index= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#odds_merged['date']= pd.to_datetime(odds_merged['date'])\n",
    "#odds_merged['scrape_time_x']= pd.to_datetime(odds_merged['scrape_time_x'])\n",
    "#odds_merged['scrape_time_y']= pd.to_datetime(odds_merged['scrape_time_y'])\n",
    "cols= [x.replace(' ','') for x in list(odds_merged.columns)]\n",
    "odds_merged.columns= cols\n",
    "odds_columns= list(odds_merged.columns)[0:76]\n",
    "col_test1= ['home_team','away_team','date','scrape_time_x','max_oddsH','max_oddsD','max_oddsA','max_BookieH','max_BookieD','max_BookieA','marginH','marginD','marginA']\n",
    "col_test2= ['home_team','away_team','date','scrape_time_x','max_oddsH2','max_oddsD2','max_oddsA2','max_BookieH2','max_BookieD2','max_BookieA2','marginH2','marginD2','marginA2']\n",
    "col_test3= ['home_team','away_team','date','scrape_time_x','max_oddsH3','max_oddsD3','max_oddsA3','max_BookieH3','max_BookieD3','max_BookieA3','marginH3','marginD3','marginA3']\n",
    "col_test4= ['home_team','away_team','date','scrape_time_x','max_oddsH4','max_oddsD4','max_oddsA4','max_BookieH4','max_BookieD4','max_BookieA4','marginH4','marginD4','marginA4']\n",
    "probls= ['home_team','away_team','date','scrape_time_x','ProbsPinnacleHome','ProbsPinnacleDraw','ProbsPinnacleAway']\n",
    "fixtures= odds_merged[['home_team','away_team','date','scrape_time_x','scrape_time_y','time_lag','difference_to_scrape','league_x','league_y']]\n",
    "odds= odds_merged[odds_columns]\n",
    "test_group1= odds_merged[col_test1]\n",
    "test_group2= odds_merged[col_test2]\n",
    "test_group3= odds_merged[col_test3]\n",
    "test_group4= odds_merged[col_test4]\n",
    "probbs= odds_merged[probls]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_username= \"root\"\n",
    "db_password= '4kS8GdBm!'\n",
    "db_ip= \"localhost\"\n",
    "db_name= 'betting'\n",
    "\n",
    "db_connection = sqlalchemy.create_engine('mysql+mysqlconnector://{0}:{1}@{2}/{3}'.\n",
    "                                               format(db_username, db_password, \n",
    "                                                      db_ip, db_name))\n",
    "\n",
    "fixtures.to_sql('fixtures',db_connection,if_exists= 'append', index= False)\n",
    "odds.to_sql('odds',db_connection,if_exists= 'append', index= False)\n",
    "test_group1.to_sql('test_group1',db_connection,if_exists= 'append', index= False)\n",
    "test_group2.to_sql('test_group2',db_connection,if_exists= 'append', index= False)\n",
    "test_group3.to_sql('test_group3',db_connection,if_exists= 'append', index= False)\n",
    "test_group4.to_sql('test_group4',db_connection,if_exists= 'append', index= False)\n",
    "probbs.to_sql('probabilities',db_connection,if_exists= 'append', index= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_db_connection(host_name, user_name, user_password, db_name):\n",
    "    connection = None\n",
    "    try:\n",
    "        connection = mysql.connector.connect(\n",
    "            host=host_name,\n",
    "            user=user_name,\n",
    "            passwd=user_password,\n",
    "            database=db_name\n",
    "        )\n",
    "        print(\"MySQL Database connection successful\")\n",
    "    except Error as err:\n",
    "        print(f\"Error: '{err}'\")\n",
    "\n",
    "    return connection\n",
    "\n",
    "def execute_query(connection, query):\n",
    "    cursor = connection.cursor()\n",
    "    try:\n",
    "        cursor.execute(query)\n",
    "        connection.commit()\n",
    "        print(\"Query successful\")\n",
    "    except Error as err:\n",
    "        print(f\"Error: '{err}'\")\n",
    "        \n",
    "\n",
    "\n",
    "def read_query(connection, query):\n",
    "    cursor = connection.cursor()\n",
    "    result = None\n",
    "    try:\n",
    "        cursor.execute(query)\n",
    "        result = cursor.fetchall()\n",
    "        return result\n",
    "    except Error as err:\n",
    "        print(f\"Error: '{err}'\")\n",
    "        \n",
    "def get_colnames(database, tablename):\n",
    "    connection = create_db_connection(\"localhost\", \"root\", '4kS8GdBm!', database)\n",
    "    cursor = connection.cursor()\n",
    "    result = None\n",
    "    try:\n",
    "        cursor.execute(\"\"\"SELECT * FROM {table}\"\"\".format(table= tablename))\n",
    "        result= [i[0] for i in cursor.description]\n",
    "        return result\n",
    "    except Error as err:\n",
    "        print(f\"Error: '{err}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "#group2.query('date <= @dated')\n",
    "#dated= pd.to_datetime(datetime.now(), format=\"%d/%m/%Y\").date()+timedelta(days=2)\n",
    "#connection = create_db_connection(\"localhost\", \"root\", '4kS8GdBm!', 'betting')\n",
    "#ll= read_query(connection, \"\"\"SELECT * FROM test_group4\"\"\")\n",
    "#group2= pd.DataFrame(ll, columns= get_colnames('betting','test_group4'))\n",
    "#today= group2[(group2['date'].dt.date == dated)].reset_index(drop= True)\n",
    "#today= today.loc[today.groupby(['home_team','away_team'])['scrape_time_x'].idxmax()].reset_index(drop= True)\n",
    "\n",
    "#today.groupby(['home_team','away_team'])['scrape_time_x'].idxmin()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#today.head(20)\n",
    "#(today[['marginH2','marginD2','marginA2']]>0)\n",
    "#positives= today.query('(marginH4 >0 & max_oddsH4< 6)| (marginD4>0 & max_oddsD4< 6) | (marginA4>0 & max_oddsA4< 6)')\n",
    "#positives.sort_values('date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "#len(spi.league.unique())\n",
    "\n",
    "#quoten= pd.DataFrame(read_query(connection, \"\"\"SELECT * FROM test_group4;\"\"\"), columns= get_colnames('betting','test_group4'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Fix the link fetcher**\n",
    "2. Get results!! \n",
    "3. **Make an analysis of lost data**\n",
    "4. **Why has France so many postive values games**? --> Rerun the analysis with the unique games"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
